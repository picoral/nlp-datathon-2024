---
title: "UA Women's+ Hackathon"
format: html
---

## Natural Language Processing with Disaster Tweets

* Log in to [kaggle.com](https://www.kaggle.com/) and access the [competition page](https://www.kaggle.com/competitions/nlp-getting-started/overview)
* Click on the black `Join Competiton` button on the top right
* We are going to be using a kaggle notebook to submit to this competition -- click on `Code` and then `+ New Notebook`
* The kaggle notebook will be created with some code for you, including code to print out the file paths to the train and test data sets
* In addition to `numpy` and `pandas`, we will be using some functions from `sklearn`

```{python}
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn import feature_extraction, linear_model, model_selection, preprocessing, svm
```

* We are going to read in the train and test file using the `read_csv` function from pandas.

```{python}
train_df = pd.read_csv("kaggle/input/nlp-getting-started/train.csv")
test_df = pd.read_csv("kaggle/input/nlp-getting-started/test.csv")
```

* We can list the variable (column) names using the `.columns` property call:

```{python}
train_df.columns
```

* We can also look at the first few observations using the `.head()` method:

```{python}
train_df.head()
```

* We can look at the first observations of a specific column by adding the column name between square brackets to the data frame:

```{python}
train_df["text"].head()
```
The target is a binary variable that is `1` when the tweet is referring to a disaster, and `0` otherwise.

```{python}
train_df.groupby("target").size()
```


## Feature extraction

* There are a number of ways to convert text to numeric variables. Let's start with scikit-learn's `CountVectorizer` to count the words in each tweet.

```{python}
count_vectorizer = feature_extraction.text.CountVectorizer()
```

```{python}
train_vectors = count_vectorizer.fit_transform(train_df["text"])
test_vectors = count_vectorizer.transform(test_df["text"])
print(train_vectors.toarray())
print(train_vectors[0])
```


* Check the [documentation page for CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)

```{python}
my_vocabulary = ["disaster", "fire", "earthquake", "shelter in place"]
count_vectorizer = feature_extraction.text.CountVectorizer(vocabulary=my_vocabulary)
```

```{python}
train_vectors = count_vectorizer.fit_transform(train_df["text"])
test_vectors = count_vectorizer.transform(test_df["text"])
print(train_vectors[0].toarray())
```

```{python}
count_vectorizer = feature_extraction.text.CountVectorizer(stop_words='english')
```

```{python}
train_vectors = count_vectorizer.fit_transform(train_df["text"])
test_vectors = count_vectorizer.transform(test_df["text"])
print(train_vectors[0].toarray())
```

Instead of counts, we can use [tf-idf (term frequency inverted document frequency)](https://www.tidytextmining.com/tfidf)

```{python}
tfid_vectorizer = feature_extraction.text.TfidfVectorizer(stop_words='english')
train_vectors_tfid = tfid_vectorizer.fit_transform(train_df["text"])
test_vectors_tfid = tfid_vectorizer.transform(test_df["text"])
print(train_vectors_tfid[0])
```

* Read the [sklearn documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text) if you want to learn about what ways to extract features from text

```{python}
hash_vectorizer = feature_extraction.text.HashingVectorizer(n_features=2**4)
train_vectors_hash = hash_vectorizer.fit_transform(train_df["text"])
test_vectors_hash = hash_vectorizer.transform(test_df["text"])
print(train_vectors_hash[0])
```


## Models

For all the models, we will evaluate a score (F1) by cross-validation.

![](images/grid_search_cross_validation.png)

![](images/github-recovery-codes.png)

### Ridge Classifier

Ridge regression

https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression

```{python}
clf = linear_model.RidgeClassifier()
```

Run the ridge classifier with the token count features.

```{python}
scores = model_selection.cross_val_score(clf, train_vectors, train_df["target"], cv=5, scoring="f1")
scores
```

Run the ridge classifier with the tf-idf features.

```{python}
scores = model_selection.cross_val_score(clf, train_vectors_tfid, train_df["target"], cv=5, scoring="f1")
scores
```

```{python}
scores = model_selection.cross_val_score(clf, train_vectors_hash, train_df["target"], cv=5, scoring="f1")
scores
```

### Logistic Regression

```{python}
logistic = linear_model.LogisticRegression()
scores = model_selection.cross_val_score(logistic, train_vectors, train_df["target"], cv=5, scoring="f1")
scores
```


### Stochastic gradient descent (SGD)

```{python}
sgd_model = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)
scores = model_selection.cross_val_score(sgd_model, train_vectors, train_df["target"], cv=3, scoring="f1")
scores
```


[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)

```{python}
sgd_model = linear_model.SGDClassifier(loss="perceptron", eta0=1, learning_rate="constant", penalty=None)
scores = model_selection.cross_val_score(sgd_model, train_vectors, train_df["target"], cv=3, scoring="f1")
scores
```


### Tree classifier

```{python}
from sklearn.tree import DecisionTreeClassifier
tree_clf = DecisionTreeClassifier(random_state=0)
scores = model_selection.cross_val_score(tree_clf, train_vectors, train_df["target"], cv=3, scoring="f1")
scores
```


### Suport Vector Machines

```{python}
support_vector = svm.SVC(gamma='auto')
scores = model_selection.cross_val_score(support_vector, train_vectors, train_df["target"], cv=2, scoring="f1")
scores
```

## Creating a submission file

```{python}
logistic.fit(train_vectors, train_df["target"])
sample_submission = pd.read_csv("kaggle/input/nlp-getting-started/sample_submission.csv")
sample_submission["target"] = logistic.predict(test_vectors)
sample_submission.to_csv("submission.csv", index=False)
```

